import time
import aiohttp
import asyncio
import aiomysql
import urllib.request
import urllib.parse
import json
from fake_useragent import UserAgent
# coding:utf-8

l_coordinate_longitude = 121.40422
r_coordinate_longitude = 121.80781

l_coordinate_latitude = 29.896064
r_coordinate_latitude = 29.759549

coordinate_list = []

#商品信息
#https://h5.ele.me/pizza/shopping/restaurants/E5255525300367143097/batch_shop?user_id=77521373&latitude=29.87386&longitude=121.55027

items_urls = ['https://h5.ele.me/restapi/shopping/v3/restaurants?latitude=29.812055&longitude=121.554408&offset={0}&limit=30']
elmurl = 'https://h5.ele.me/restapi/shopping/v3/restaurants?latitude={0}&longitude={1}'
elmitems_urls=[]
item_urls = []
failure_urls = []
retry_urls = []
retry_comment_urls = []
request_urls = []
stopping = False
offset = 0
num = 0
failure_time = 0
retry_time = 0

headers = {
    "user-agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1",
    "referer": "https://h5.ele.me/",
    "cookie": "ubt_ssid=6dvazct83jsltgnxvvgagjnz69o38ssc_2019-01-17; _utrace=799bdd4c3e60cd5c80f476eb0bec41c0_2019-01-17; perf_ssid=menh6ykr9ae4nrkqdduch0hihapypzh2_2019-01-17; cna=yZvrE2umezECAXPWiWCVnpoV; track_id=1547792339|a2d1a8fa34aea41d4e6df70029df505a339923f23a5975b549|05536f807f3a50f41f692f43bbb47303; USERID=5718424762; UTUSER=5718424762; SID=nAvg7QrkX9bcZFhIhxIcHc76fDpexLcnZg8A; isg=BAAA_kHISLCTEjT4-wWbBt7d0Y4SIeXLzFvD9nqRzZuu9aEfIpki64jECVs1xZwr",
    "upgrade-insecure-requests": "1"
}

headersshop = {
    "user-agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1",
    "referer": "https://h5.ele.me/shop/",
    "cookie": "ubt_ssid=6dvazct83jsltgnxvvgagjnz69o38ssc_2019-01-17; _utrace=799bdd4c3e60cd5c80f476eb0bec41c0_2019-01-17; perf_ssid=menh6ykr9ae4nrkqdduch0hihapypzh2_2019-01-17; cna=yZvrE2umezECAXPWiWCVnpoV; track_id=1547792339|a2d1a8fa34aea41d4e6df70029df505a339923f23a5975b549|05536f807f3a50f41f692f43bbb47303; USERID=5718424762; UTUSER=5718424762; SID=nAvg7QrkX9bcZFhIhxIcHc76fDpexLcnZg8A; isg=BAAA_kHISLCTEjT4-wWbBt7d0Y4SIeXLzFvD9nqRzZuu9aEfIpki64jECVs1xZwr",
    "upgrade-insecure-requests": "1"
}


start_time = time.time()
PROXY_POOL_URL = 'http://112.85.167.48:9999'
sem = asyncio.Semaphore(100)


def get_one_coordinate():
    lx = l_coordinate_longitude
    ly = l_coordinate_latitude
    for i in range(1, 200):
        if lx < r_coordinate_longitude:
            coordinate_list.append([lx, ly])
            lx = lx + 0.03
        else:
            ly = ly - 0.03
            lx = l_coordinate_longitude

        if ly < r_coordinate_latitude:
            break

async def get_proxy(s):
        try:
            async with s.get(PROXY_POOL_URL) as resp:
                if resp.status == 200:
                    proxy = await resp.text(encoding="utf-8")
                    print("使用代理：%s" % proxy)
                    return 'http://' + proxy
        except ConnectionError:
            return None

async def retry(s, url, loop):
    global retry_time
    while retry_time < 6:
        if len(retry_urls) == 0:
            await asyncio.sleep(0.5)
            continue

        async with s.get(url, headers=headers, proxy=await get_proxy(s)) as resp:
            if resp.status in [200, 201]:
                result = await resp.json()
                text_json = result.get('items')
                if text_json is not None:
                    await parse_page(text_json, loop, s)
                    break
            else:
                retry_time += 1
                print("retry url:%s,retry time:%s" % (url, retry_time))

    if retry_time >= 6:
        failure_urls.append(url)
        print("failure url:", url)

async def area_url(loop):
    get_one_coordinate()
    for xy in range(1, len(coordinate_list)):
        elmitems_urls.append(elmurl.format(coordinate_list[xy][1], coordinate_list[xy][0]) + '&offset={0}&limit=30')

    for itea in range(1, len(elmitems_urls)):
        items_urls[0] = elmitems_urls[itea]
        global num
        num = 0
        await request_page(loop)



async def request_page(loop):
    async with sem:
        while not stopping:
            try:
                global num
                offset = num * 30
                num += 1
                print("Page:%s" % offset)
                async with aiohttp.ClientSession() as session:
                    url = items_urls[0].format(offset)
                    print("Request url:%s" % url)
                    async with session.get(url, headers=headers) as resp:
                        if resp.status in [200, 201]:
                            result = await resp.json()
                            text_json = result.get('items')
                            if len(text_json) > 0:
                                print(text_json)
                                request_urls.append(url)
                                await parse_page(text_json, loop, session)
                            else:
                                print(text_json)
                                print('there is no content in the next page')
                                break
                        else:
                            retry_urls.append(url)
                            await retry(session, url, loop)
                            print('failure url:%s,failure code:%s' % (url, resp.status))

            except Exception as e:
                print("[Info] Exception:", e)

            time.sleep(5)

async def parse_page(text_json, loop, s):
    for one in text_json:
        item = one.get('restaurant')
        authentic_id = item.get('authentic_id')
        shopid = item.get('id')
        shopname = item.get('name')

        if await get_shopinfo_byshopname(shopname, loop) == 1:
            print("数据已记录 跳过当前记录")
            continue

        rating = item.get('rating')
        recent_order_num = item.get('recent_order_num')
        opening_hours = item.get('opening_hours')[0] if item.get('opening_hours') else ''
        address = item.get('address')
        flavors_id = ' '.join([str(item.get('id')) for item in item.get('flavors')]) if item.get('flavors') else ''
        flavors_name = ' '.join([item.get('name') for item in item.get('flavors')]) if item.get('flavors') else ''
        minimum_order_amount = item.get('float_minimum_order_amount')
        delivery_fee = item.get('float_delivery_fee')
        distance = item.get('distance')
        order_lead_time = item.get('order_lead_time')
        support_tags = ' '.join([item.get('text') for item in item.get('support_tags')]) if item.get('support_tags') else ''
        recommend_reasons = ' '.join([item.get('name') for item in item.get('recommend_reasons')]) if item.get('recommend_reasons') else ''
        recommend = item.get('recommend').get('reason')
        description = item.get('description')
        promotion_info = item.get('promotion_info')
        phone = item.get('phone')
        folding_restaurant_brand = item.get('folding_restaurant_brand') if item.get('folding_restaurant_brand') else ''
        folding_restaurants_id = ' '.join([str(item.get('id')) for item in item.get('folding_restaurants')]) if item.get('folding_restaurant_brand') else ''
        folding_restaurants_name = ' '.join([item.get('name') for item in item.get('folding_restaurants')]) if item.get('folding_restaurant_brand') else ''
        latitude = item.get('latitude')
        longitude = item.get('longitude')
        crawl_time = time.strftime('%Y-%m-%d', time.localtime())
        dataarr = []
        data = (authentic_id, shopid, shopname, rating, recent_order_num, opening_hours, address,
                flavors_id, flavors_name, minimum_order_amount, delivery_fee, distance,
                order_lead_time, support_tags, recommend_reasons, recommend, description,
                promotion_info, phone, folding_restaurant_brand, folding_restaurants_id, folding_restaurants_name,
                latitude, longitude, crawl_time)

        dataarr.append(data)

        insert_sql = """
                        insert into t_elmbusiness(E_authentic_id, E_shopid, E_shopname, E_rating, E_recent_order_num, E_opening_hours, E_address,
                                                E_flavors_id, E_flavors_name, E_minimum_order_amount, E_delivery_fee, E_distance,
                                                E_order_lead_time, E_support_tags, E_recommend_reasons, E_recommend, E_description,
                                                E_promotion_info, E_phone, E_folding_restaurant_brand, E_folding_restaurants_id, E_folding_restaurants_name,
                                                E_latitude, E_longitude, E_crawl_time)
                        values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """

        await save_into_mysql(insert_sql, dataarr, loop,'店铺信息')
        await parse_shopfoodinfo(s, shopid, latitude, longitude)
        time.sleep(5)


        #await parse_comment(s, shopid)
#抓取店铺商品数据
async def parse_shopfoodinfo(s, shopid, latitude, longitude):
    url = 'https://h5.ele.me/pizza/shopping/restaurants/{0}/batch_shop?user_id=5718424762&latitude={1}&longitude={2}'.format(shopid, latitude, longitude)
    print(url)
    try:
        async with s.get(url, headers=headersshop) as resp:
            if resp.status in [200, 201]:
                print("返回数据 200 ====")
                result = await resp.json()
                menu_list = result.get('menu')
                foodsarr = []
                for once in menu_list:
                    food_list = once.get('foods')
                    for food in food_list:
                        image_path = food.get('image_path')
                        description = food.get('description')
                        item_id = food.get('item_id')
                        month_sales = food.get('month_sales')
                        name = food.get('name')
                        rating = food.get('rating')
                        rating_count = food.get('rating_count')
                        satisfy_count = food.get('satisfy_count')
                        satisfy_rate = food.get('satisfy_rate')
                        tips = food.get('tips')
                        original_price = food.get('specfoods')[0].get('original_price')
                        packing_fee = food.get('specfoods')[0].get('packing_fee')
                        price = food.get('specfoods')[0].get('price')
                        data = (shopid,image_path,description,item_id,month_sales,name,rating,rating_count,satisfy_count,satisfy_rate,tips,original_price,packing_fee,price)
                        foodsarr.append(data)

                insert_sql = """
                            insert into t_elmbusinessfoods(E_shopid, E_image_path, E_description, E_item_id, E_month_sales, E_name, E_rating,
                                               E_rating_count, E_satisfy_count, E_satisfy_rate, E_tips, E_original_price,
                                               E_packing_fee, E_price)
                            values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                         """
                await save_into_mysql(insert_sql, foodsarr, loop, '商品信息')
                time.sleep(1)
            else:
                print('failure url:%s,failure code:%s' % (url, resp.status))
    except Exception as e:
        print("[Info] Exception:", e)
#抓取评论
async def parse_comment(s, shopid):
    url = 'https://h5.ele.me/restapi/ugc/v3/restaurants/{0}/ratings?has_content=true&tag_name=%E5%85%A8%E9%83%A8&offset=0&limit=100000'.format(shopid)
    async with s.get(url, headers=headers) as resp:
        if resp.status in [200, 201]:
            comment_list = await resp.json()
            rated_datetime = [item.get('rated_at') for item in comment_list]
            score = [item.get('rating') for item in comment_list]
            content = list(filter(None, [item.get('rating_text').strip() for item in comment_list]))
            crawl_time = time.strftime('%Y-%m-%d', time.localtime())
            data = {
                "rated_datetime": rated_datetime,
                "score": score,
                "content": content,
                "crawl_time": crawl_time
            }
           # await save_into_mongodb(data)
        else:
            retry_comment_urls.append(url)
            await retry(s, url, loop)
            print('failure url:%s,failure code:%s' % (url, resp.status))


# async def save_into_mongodb(data):
#     print('正在写入MongoDB')
#     client = pymongo.MongoClient(host='127.0.0.1', port=27017)
#     elemedb = client['eleme']                           # 给数据库命名
#     collection = elemedb['eleme_guangzhou_comment']     # 创建数据表
#     collection.insert_one(data)
#     print('写入MongoDB完成')
#

async def get_shopinfo_byshopname(shopname,loop):
    quyer_sql = 'select * from t_elmbusiness where E_shopname = "{0}" '.format(shopname)
    res = 0
    pool = await aiomysql.create_pool(host='127.0.0.1', port=3306,
                                      user='root', password='Rocwei19940106',
                                      db='spider_elm', loop=loop, charset='utf8', autocommit=True)
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            await cur.execute(quyer_sql)
            r = await cur.fetchall()
            if not r:
                res = 0
            else:
                res = 1
    pool.close()
    await pool.wait_closed()
    return res

async def save_into_mysql(insert_sql, data, loop, typename):
    pool = await aiomysql.create_pool(host='127.0.0.1', port=3306,
                                      user='root', password='Rocwei19940106',
                                      db='spider_elm', loop=loop, charset='utf8', autocommit=True)
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            print('正在写入MySQL'+ typename)
            await cur.executemany(insert_sql, data)
            print('写入MySQL完成'+ typename)
    pool.close()
    await pool.wait_closed()


if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    task = asyncio.ensure_future(area_url(loop))
    loop.run_until_complete(task)
    loop.close()
    print("*"*20, "END INFO", "*"*20)
    print("total request url:", len(request_urls))
    print("retry url counts:%s" % len(retry_urls))
    print("failure url counts:%s" % len(failure_urls))
    print("Consumed time:", time.time() - start_time)
